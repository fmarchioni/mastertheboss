# AI
quarkus.langchain4j.chat-model.provider=ollama
# ollama
quarkus.langchain4j.ollama.base-url=http://localhost:11434/
quarkus.langchain4j.ollama.log-requests=false
quarkus.langchain4j.ollama.log-responses=false
quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.embedding-model.model-id=3b
quarkus.langchain4j.ollama.timeout=180s
# Example structured prompt configuration
quarkus.langchain4j.ollama.chat-model.temperature=0.7
quarkus.langchain4j.ollama.chat-model.max-tokens=512


